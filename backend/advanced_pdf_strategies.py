"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF —Ñ–∞–π–ª–æ–≤
1. –°—Ç—Ä–∞—Ç–µ–≥–∏—è –£–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ –ó–∞–ø—Ä–æ—Å–∞ (Controlled HTTP Requests)
2. –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ü–µ—Ä–µ—Ö–≤–∞—Ç–∞ –°–µ—Ç–µ–≤–æ–≥–æ –¢—Ä–∞—Ñ–∏–∫–∞ (Network Interception)
"""

import os
import time
import logging
import requests
import hashlib
import re
from urllib.parse import urljoin, urlparse
from typing import List, Optional, Dict, Any

logger = logging.getLogger(__name__)

# –ß–ï–†–ù–´–ô –°–ü–ò–°–û–ö URL - –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –Ω—É–∂–Ω–æ —Å–∫–∞—á–∏–≤–∞—Ç—å
PDF_URL_BLACKLIST = [
    'Content/–ü–æ–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏.pdf',
    '/Content/–ü–æ–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏.pdf',
    'https://kad.arbitr.ru/Content/–ü–æ–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏.pdf',
    'privacy',
    'policy',
    'terms',
    'agreement',
    'cookie',
    'help',
    'manual',
    'instruction'
]

def is_blacklisted_url(url):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ URL –≤ —á–µ—Ä–Ω–æ–º —Å–ø–∏—Å–∫–µ
    
    Args:
        url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        
    Returns:
        True –µ—Å–ª–∏ URL –≤ —á–µ—Ä–Ω–æ–º —Å–ø–∏—Å–∫–µ
    """
    if not url:
        return True
    
    url_lower = url.lower()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫
    for blacklisted in PDF_URL_BLACKLIST:
        if blacklisted.lower() in url_lower:
            logger.warning(f"üö´ [BLACKLIST] URL –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {url}")
            logger.warning(f"üö´ [BLACKLIST] –ü—Ä–∏—á–∏–Ω–∞: —Å–æ–¥–µ—Ä–∂–∏—Ç '{blacklisted}'")
            return True
    
    return False

def is_case_document_url(url):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –¥–æ–∫—É–º–µ–Ω—Ç–æ–º –¥–µ–ª–∞ (–∞ –Ω–µ —Å–ª—É–∂–µ–±–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º)
    
    Args:
        url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        
    Returns:
        True –µ—Å–ª–∏ —ç—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç –¥–µ–ª–∞
    """
    if not url:
        return False
    
    url_lower = url.lower()
    
    # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–µ–ª–∞
    case_indicators = [
        'document/pdf',
        'kad/pdfdocument',
        'pdfdocument',
        'document/getpdf',
        'getpdf',
        '/card/',
        'caseid',
        'documentid'
    ]
    
    for indicator in case_indicators:
        if indicator in url_lower:
            logger.info(f"‚úÖ [CASE DOC] –î–æ–∫—É–º–µ–Ω—Ç –¥–µ–ª–∞: {url} (–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä: {indicator})")
            return True
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –Ω–æ –µ—Å—Ç—å GUID-–ø–æ–¥–æ–±–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    guid_pattern = r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}'
    if re.search(guid_pattern, url_lower, re.IGNORECASE):
        logger.info(f"‚úÖ [CASE DOC] –î–æ–∫—É–º–µ–Ω—Ç —Å GUID: {url}")
        return True
    
    logger.debug(f"‚ö†Ô∏è [NOT CASE] –ù–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç –¥–µ–ª–∞: {url}")
    return False

class ControlledHTTPStrategy:
    """
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è –£–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ –ó–∞–ø—Ä–æ—Å–∞ (Controlled HTTP Requests)
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç requests –¥–ª—è –ø—Ä—è–º–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF —Ñ–∞–π–ª–æ–≤
    """
    
    def __init__(self, files_dir: str):
        self.files_dir = files_dir
        os.makedirs(files_dir, exist_ok=True)
        self.session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.6843.82 Safari/537.36',
            'Accept': 'application/pdf,application/octet-stream,*/*',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'DNT': '1'
        })
    
    def set_referer(self, referer_url: str):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç Referer –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞ —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–ª–∞"""
        self.session.headers['Referer'] = referer_url
        logger.info(f"üîó –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω Referer: {referer_url}")
    
    def set_cookies_from_browser(self, cookies: List[Dict[str, Any]]):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç cookies –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞"""
        for cookie in cookies:
            try:
                self.session.cookies.set(
                    name=cookie['name'],
                    value=cookie['value'],
                    domain=cookie.get('domain', ''),
                    path=cookie.get('path', '/')
                )
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ cookie {cookie.get('name', 'unknown')}: {e}")
        
        logger.info(f"üç™ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {len(cookies)} cookies")
    
    def download_pdf(self, pdf_url: str, case_url: str, filename_prefix: str = "controlled") -> Optional[str]:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç PDF —Ñ–∞–π–ª —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ HTTP –∑–∞–ø—Ä–æ—Å–∞
        
        Args:
            pdf_url: URL PDF —Ñ–∞–π–ª–∞
            case_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–ª–∞ (–¥–ª—è Referer)
            filename_prefix: –ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            
        Returns:
            –ü—É—Ç—å –∫ —Å–∫–∞—á–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ü–†–û–í–ï–†–ö–ê –ß–ï–†–ù–û–ì–û –°–ü–ò–°–ö–ê
            if is_blacklisted_url(pdf_url):
                logger.error(f"‚ùå [BLOCKED] –ü–æ–ø—ã—Ç–∫–∞ —Å–∫–∞—á–∞—Ç—å URL –∏–∑ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞: {pdf_url}")
                return None
            
            # –ü–†–û–í–ï–†–ö–ê: –î–æ–∫—É–º–µ–Ω—Ç –¥–µ–ª–∞ –∏–ª–∏ —Å–ª—É–∂–µ–±–Ω—ã–π
            if not is_case_document_url(pdf_url):
                logger.warning(f"‚ö†Ô∏è [NOT CASE] –ü–æ–ø—ã—Ç–∫–∞ —Å–∫–∞—á–∞—Ç—å –Ω–µ-–¥–æ–∫—É–º–µ–Ω—Ç –¥–µ–ª–∞: {pdf_url}")
                return None
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Referer
            self.set_referer(case_url)
            
            logger.info(f"üì• [DOWNLOAD] –£–ø—Ä–∞–≤–ª—è–µ–º—ã–π –∑–∞–ø—Ä–æ—Å: {pdf_url}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —Å –ø–æ—Ç–æ–∫–æ–≤—ã–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º
            response = self.session.get(pdf_url, stream=True, timeout=30)
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type
            content_type = response.headers.get('content-type', '').lower()
            if 'pdf' not in content_type:
                logger.warning(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π Content-Type: {content_type}")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
                first_chunk = next(response.iter_content(1024), b'')
                if not first_chunk.startswith(b'%PDF'):
                    logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è PDF: {pdf_url}")
                    return None
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
            url_hash = hashlib.md5(pdf_url.encode()).hexdigest()[:8]
            filename = f"{filename_prefix}_{url_hash}.pdf"
            filepath = os.path.join(self.files_dir, filename)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
            if os.path.exists(filepath):
                existing_size = os.path.getsize(filepath)
                if existing_size == int(response.headers.get('content-length', 0)):
                    logger.info(f"‚ÑπÔ∏è –§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {filename}")
                    return filepath
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
            total_size = 0
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        total_size += len(chunk)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            if total_size < 1000:  # –ú–∏–Ω–∏–º—É–º 1KB
                logger.warning(f"‚ö†Ô∏è –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª: {total_size} –±–∞–π—Ç")
                os.remove(filepath)
                return None
            
            logger.info(f"‚úÖ PDF —Å–∫–∞—á–∞–Ω: {filename} ({total_size} –±–∞–π—Ç)")
            return filepath
            
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
            return None
    
    def find_pdf_urls_in_html(self, html_content: str, base_url: str) -> List[str]:
        """
        –ò—â–µ—Ç PDF URL –≤ HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
        
        Args:
            html_content: HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            base_url: –ë–∞–∑–æ–≤—ã–π URL –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö PDF URL
        """
        pdf_urls = []
        blocked_count = 0
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ PDF URL
        patterns = [
            r'href=["\']([^"\']*\.pdf[^"\']*)["\']',
            r'src=["\']([^"\']*\.pdf[^"\']*)["\']',
            r'url["\']?:\s*["\']([^"\']+\.pdf[^"\']*)["\']',
            r'file["\']?:\s*["\']([^"\']+\.pdf[^"\']*)["\']',
            r'data-pdf=["\']([^"\']*\.pdf[^"\']*)["\']',
            r'data-file=["\']([^"\']*\.pdf[^"\']*)["\']',
            r'data-url=["\']([^"\']*\.pdf[^"\']*)["\']',
            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è kad.arbitr.ru
            r'Document/Pdf[^"\']*',
            r'Kad/PdfDocument[^"\']*',
            r'PdfDocument[^"\']*',
            r'Document/GetPdf[^"\']*',
            r'GetPdf[^"\']*'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else match[1]
                
                url = match.strip()
                if url and len(url) > 5:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                    if not url.startswith('http'):
                        if url.startswith('/'):
                            url = urljoin(base_url, url)
                        else:
                            url = urljoin(base_url + '/', url)
                    
                    # –ü–†–û–í–ï–†–ö–ê –ß–ï–†–ù–û–ì–û –°–ü–ò–°–ö–ê
                    if is_blacklisted_url(url):
                        blocked_count += 1
                        continue
                    
                    # –ü–†–û–í–ï–†–ö–ê: –î–æ–∫—É–º–µ–Ω—Ç –¥–µ–ª–∞ –∏–ª–∏ —Å–ª—É–∂–µ–±–Ω—ã–π
                    if not is_case_document_url(url):
                        logger.debug(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω URL (–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç –¥–µ–ª–∞): {url}")
                        continue
                    
                    if url not in pdf_urls:
                        pdf_urls.append(url)
                        logger.info(f"‚úÖ [FOUND] –ù–∞–π–¥–µ–Ω PDF URL: {url}")
        
        if blocked_count > 0:
            logger.warning(f"üö´ [BLOCKED] –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ {blocked_count} URL –∏–∑ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞")
        
        return pdf_urls


class NetworkInterceptionStrategy:
    """
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ü–µ—Ä–µ—Ö–≤–∞—Ç–∞ –°–µ—Ç–µ–≤–æ–≥–æ –¢—Ä–∞—Ñ–∏–∫–∞ (Network Interception)
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Playwright –¥–ª—è –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞ —Å–µ—Ç–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    """
    
    def __init__(self, files_dir: str):
        self.files_dir = files_dir
        os.makedirs(files_dir, exist_ok=True)
        self.intercepted_requests = []
    
    def setup_interception(self, page):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–µ—Ä–µ—Ö–≤–∞—Ç —Å–µ—Ç–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        
        Args:
            page: Playwright page –æ–±—ä–µ–∫—Ç
        """
        logger.info("üï∏Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞ —Å–µ—Ç–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        def handle_route(route):
            """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
            request = route.request
            url = request.url
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å PDF
            if self._is_pdf_request(request):
                logger.info(f"üéØ –ü–µ—Ä–µ—Ö–≤–∞—á–µ–Ω PDF –∑–∞–ø—Ä–æ—Å: {url}")
                self.intercepted_requests.append({
                    'url': url,
                    'method': request.method,
                    'headers': request.headers,
                    'timestamp': time.time()
                })
            
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–ø—Ä–æ—Å
            route.continue_()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–µ—Ä–µ—Ö–≤–∞—Ç –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        page.route("**/*", handle_route)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ PDF –∑–∞–ø—Ä–æ—Å—ã
        page.route("**/*.pdf", handle_route)
        page.route("**/*Document*", handle_route)
        page.route("**/*Pdf*", handle_route)
    
    def _is_pdf_request(self, request) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å PDF —Ñ–∞–π–ª–æ–º
        
        Args:
            request: Playwright request –æ–±—ä–µ–∫—Ç
            
        Returns:
            True –µ—Å–ª–∏ —ç—Ç–æ PDF –∑–∞–ø—Ä–æ—Å
        """
        url = request.url.lower()
        headers = request.headers
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
        url_indicators = [
            '.pdf',
            'document/pdf',
            'kad/pdfdocument',
            'pdfdocument',
            'getpdf'
        ]
        
        url_match = any(indicator in url for indicator in url_indicators)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        accept_header = headers.get('accept', '').lower()
        content_type = headers.get('content-type', '').lower()
        
        header_match = (
            'application/pdf' in accept_header or
            'application/pdf' in content_type or
            'application/octet-stream' in accept_header
        )
        
        return url_match or header_match
    
    async def intercept_pdf_downloads(self, page, case_url: str, timeout: int = 30) -> List[str]:
        """
        –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç PDF –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        
        Args:
            page: Playwright page –æ–±—ä–µ–∫—Ç
            case_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–ª–∞
            timeout: –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–∫–∞—á–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        """
        logger.info(f"üåê –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {case_url}")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–µ—Ä–µ—Ö–≤–∞—Ç
        self.setup_interception(page)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        await page.goto(case_url, wait_until='networkidle')
        
        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ PDF
        logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ PDF –∑–∞–≥—Ä—É–∑–æ–∫ ({timeout}—Å)...")
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            if self.intercepted_requests:
                break
            await page.wait_for_timeout(1000)  # –ñ–¥–µ–º 1 —Å–µ–∫—É–Ω–¥—É
        
        downloaded_files = []
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–Ω—ã–µ PDF
        for i, req_info in enumerate(self.intercepted_requests):
            try:
                logger.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF {i+1}: {req_info['url']}")
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —Å —Ç–µ–º–∏ –∂–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                response = await page.request.get(req_info['url'])
                
                if response.status == 200:
                    content = await response.body()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ PDF
                    if content.startswith(b'%PDF'):
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                        url_hash = hashlib.md5(req_info['url'].encode()).hexdigest()[:8]
                        filename = f"intercepted_{url_hash}.pdf"
                        filepath = os.path.join(self.files_dir, filename)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
                        with open(filepath, 'wb') as f:
                            f.write(content)
                        
                        downloaded_files.append(filepath)
                        logger.info(f"‚úÖ PDF —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename} ({len(content)} –±–∞–π—Ç)")
                    else:
                        logger.warning(f"‚ö†Ô∏è –ù–µ PDF —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ: {req_info['url']}")
                else:
                    logger.warning(f"‚ö†Ô∏è HTTP {response.status}: {req_info['url']}")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF {i+1}: {e}")
                continue
        
        return downloaded_files
    
    def get_intercepted_requests(self) -> List[Dict[str, Any]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        return self.intercepted_requests.copy()


class AdvancedPDFExtractor:
    """
    –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä PDF —Ñ–∞–π–ª–æ–≤
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, files_dir: str):
        self.files_dir = files_dir
        self.http_strategy = ControlledHTTPStrategy(files_dir)
        self.interception_strategy = NetworkInterceptionStrategy(files_dir)
    
    def extract_with_controlled_http(self, case_url: str, html_content: str, cookies: List[Dict[str, Any]]) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç PDF –∏—Å–ø–æ–ª—å–∑—É—è —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ HTTP –∑–∞–ø—Ä–æ—Å—ã
        
        Args:
            case_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–ª–∞
            html_content: HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            cookies: Cookies –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–∫–∞—á–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        """
        logger.info("üöÄ –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –£–ø—Ä–∞–≤–ª—è–µ–º—ã–µ HTTP –∑–∞–ø—Ä–æ—Å—ã")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º cookies
        self.http_strategy.set_cookies_from_browser(cookies)
        
        # –ò—â–µ–º PDF URL –≤ HTML
        logger.info("üîç –ü–æ–∏—Å–∫ PDF URL –≤ HTML...")
        pdf_urls = self.http_strategy.find_pdf_urls_in_html(html_content, case_url)
        
        if not pdf_urls:
            logger.warning("‚ö†Ô∏è [NO URLS] PDF URL –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ HTML (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)")
            return []
        
        logger.info(f"üìã [FOUND] –ù–∞–π–¥–µ–Ω–æ {len(pdf_urls)} –≤–∞–ª–∏–¥–Ω—ã—Ö PDF URL (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)")
        
        downloaded_files = []
        blocked_count = 0
        success_count = 0
        failed_count = 0
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π PDF
        for i, pdf_url in enumerate(pdf_urls):
            try:
                logger.info(f"üì• [{i+1}/{len(pdf_urls)}] –ü–æ–ø—ã—Ç–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {pdf_url}")
                
                filepath = self.http_strategy.download_pdf(
                    pdf_url, 
                    case_url, 
                    f"controlled_{i+1}"
                )
                
                if filepath:
                    downloaded_files.append(filepath)
                    success_count += 1
                    logger.info(f"‚úÖ [{i+1}/{len(pdf_urls)}] PDF —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω: {os.path.basename(filepath)}")
                else:
                    failed_count += 1
                    logger.warning(f"‚ùå [{i+1}/{len(pdf_urls)}] PDF –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å")
                    
            except Exception as e:
                failed_count += 1
                logger.error(f"‚ùå [{i+1}/{len(pdf_urls)}] –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
                continue
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info(f"üìä [STATS] HTTP —Å—Ç—Ä–∞—Ç–µ–≥–∏—è:")
        logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count}")
        logger.info(f"   ‚ùå –û—à–∏–±–∫–∏: {failed_count}")
        logger.info(f"   üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(downloaded_files)}")
        
        return downloaded_files
    
    async def extract_with_network_interception(self, page, case_url: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç PDF –∏—Å–ø–æ–ª—å–∑—É—è –ø–µ—Ä–µ—Ö–≤–∞—Ç —Å–µ—Ç–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞
        
        Args:
            page: Playwright page –æ–±—ä–µ–∫—Ç
            case_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–ª–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–∫–∞—á–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        """
        logger.info("üöÄ –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–µ—Ä–µ—Ö–≤–∞—Ç —Å–µ—Ç–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞")
        
        try:
            downloaded_files = await self.interception_strategy.intercept_pdf_downloads(
                page, case_url, timeout=30
            )
            
            if downloaded_files:
                logger.info(f"‚úÖ –ü–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–æ {len(downloaded_files)} PDF —Ñ–∞–π–ª–æ–≤")
            else:
                logger.warning("‚ö†Ô∏è PDF —Ñ–∞–π–ª—ã –Ω–µ –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω—ã")
            
            return downloaded_files
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞: {e}")
            return []
    
    async def extract_with_both_strategies(self, page, case_url: str, html_content: str, cookies: List[Dict[str, Any]]) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç PDF –∏—Å–ø–æ–ª—å–∑—É—è –æ–±–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        
        Args:
            page: Playwright page –æ–±—ä–µ–∫—Ç
            case_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–ª–∞
            html_content: HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            cookies: Cookies –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–∫–∞—á–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        """
        logger.info("üöÄ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: HTTP + –ü–µ—Ä–µ—Ö–≤–∞—Ç")
        
        all_files = []
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –£–ø—Ä–∞–≤–ª—è–µ–º—ã–µ HTTP –∑–∞–ø—Ä–æ—Å—ã
        try:
            http_files = self.extract_with_controlled_http(case_url, html_content, cookies)
            all_files.extend(http_files)
            logger.info(f"üìÑ HTTP —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {len(http_files)} —Ñ–∞–π–ª–æ–≤")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ HTTP —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–µ—Ä–µ—Ö–≤–∞—Ç —Å–µ—Ç–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞
        try:
            interception_files = await self.extract_with_network_interception(page, case_url)
            all_files.extend(interception_files)
            logger.info(f"üìÑ –ü–µ—Ä–µ—Ö–≤–∞—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {len(interception_files)} —Ñ–∞–π–ª–æ–≤")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞: {e}")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_files = list(set(all_files))
        logger.info(f"üéâ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(unique_files)}")
        
        return unique_files
