#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è PDF —Ñ–∞–π–ª–æ–≤
"""

import os
import time
import logging
import requests
import json
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import tempfile
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

logger = logging.getLogger(__name__)

class PDFExtractionAlgorithms:
    """–ö–ª–∞—Å—Å —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è PDF"""
    
    def __init__(self, driver, files_dir):
        self.driver = driver
        self.files_dir = files_dir
        self.downloaded_files = []
    
    def extract_pdfs_algorithm_1_direct_links(self, case_number):
        """–ê–ª–≥–æ—Ä–∏—Ç–º 1: –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF —á–µ—Ä–µ–∑ data-–∞—Ç—Ä–∏–±—É—Ç—ã"""
        logger.info("üîç –ê–ª–≥–æ—Ä–∏—Ç–º 1: –ü–æ–∏—Å–∫ –ø—Ä—è–º—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ PDF")
        
        try:
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å data-–∞—Ç—Ä–∏–±—É—Ç–∞–º–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF
            pdf_elements = self.driver.find_elements(By.CSS_SELECTOR, 
                '[data-pdf], [data-file], [data-document], [data-url]')
            
            pdf_links = []
            for element in pdf_elements:
                for attr in ['data-pdf', 'data-file', 'data-document', 'data-url']:
                    if element.get_attribute(attr):
                        pdf_links.append(element.get_attribute(attr))
            
            # –¢–∞–∫–∂–µ –∏—â–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Å—Å—ã–ª–∫–∏
            hidden_links = self.driver.find_elements(By.CSS_SELECTOR, 
                'a[href*=".pdf"]:not([style*="display: none"])')
            
            for link in hidden_links:
                href = link.get_attribute('href')
                if href and href not in pdf_links:
                    pdf_links.append(href)
            
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_links)} –ø—Ä—è–º—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ PDF")
            return self._download_files(pdf_links, "algorithm_1")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ 1: {e}")
            return []
    
    def extract_pdfs_algorithm_2_javascript_links(self, case_number):
        """–ê–ª–≥–æ—Ä–∏—Ç–º 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ JavaScript"""
        logger.info("üîç –ê–ª–≥–æ—Ä–∏—Ç–º 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ JavaScript")
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º JavaScript –¥–ª—è –ø–æ–∏—Å–∫–∞ PDF —Å—Å—ã–ª–æ–∫
            js_code = """
            var pdfLinks = [];
            
            // –ü–æ–∏—Å–∫ –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
            var elements = document.querySelectorAll('[data-pdf], [data-file], [data-document]');
            elements.forEach(function(el) {
                ['data-pdf', 'data-file', 'data-document'].forEach(function(attr) {
                    var value = el.getAttribute(attr);
                    if (value && value.includes('.pdf')) {
                        pdfLinks.push(value);
                    }
                });
            });
            
            // –ü–æ–∏—Å–∫ –≤ onclick —Å–æ–±—ã—Ç–∏—è—Ö
            var onclickElements = document.querySelectorAll('[onclick*="pdf"], [onclick*="download"]');
            onclickElements.forEach(function(el) {
                var onclick = el.getAttribute('onclick');
                var match = onclick.match(/['"]([^'"]*\.pdf[^'"]*)['"]/);
                if (match) {
                    pdfLinks.push(match[1]);
                }
            });
            
            // –ü–æ–∏—Å–∫ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Ñ–æ—Ä–º–∞—Ö
            var forms = document.querySelectorAll('form');
            forms.forEach(function(form) {
                var inputs = form.querySelectorAll('input[type="hidden"]');
                inputs.forEach(function(input) {
                    var value = input.value;
                    if (value && value.includes('.pdf')) {
                        pdfLinks.push(value);
                    }
                });
            });
            
            return pdfLinks;
            """
            
            pdf_links = self.driver.execute_script(js_code)
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_links)} —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ JavaScript")
            return self._download_files(pdf_links, "algorithm_2")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ 2: {e}")
            return []
    
    def extract_pdfs_algorithm_3_api_calls(self, case_number):
        """–ê–ª–≥–æ—Ä–∏—Ç–º 3: –ü–æ–∏—Å–∫ API –≤—ã–∑–æ–≤–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è PDF"""
        logger.info("üîç –ê–ª–≥–æ—Ä–∏—Ç–º 3: –ü–æ–∏—Å–∫ API –≤—ã–∑–æ–≤–æ–≤")
        
        try:
            # –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º —Å–µ—Ç–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            self.driver.execute_cdp_cmd('Network.enable', {})
            
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–∑—ã–≤–∞—Ç—å API
            api_elements = self.driver.find_elements(By.CSS_SELECTOR, 
                '[onclick*="ajax"], [onclick*="fetch"], [onclick*="XMLHttpRequest"]')
            
            pdf_links = []
            for element in api_elements:
                try:
                    # –ö–ª–∏–∫–∞–µ–º –∏ –∂–¥–µ–º –æ—Ç–≤–µ—Ç–∞
                    element.click()
                    time.sleep(2)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–∏ —Å–µ—Ç–∏
                    logs = self.driver.get_log('performance')
                    for log in logs:
                        message = json.loads(log['message'])
                        if message['message']['method'] == 'Network.responseReceived':
                            url = message['message']['params']['response']['url']
                            if '.pdf' in url.lower():
                                pdf_links.append(url)
                                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ API —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                    continue
            
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_links)} —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ API")
            return self._download_files(pdf_links, "algorithm_3")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ 3: {e}")
            return []
    
    def extract_pdfs_algorithm_4_dynamic_content(self, case_number):
        """–ê–ª–≥–æ—Ä–∏—Ç–º 4: –ü–æ–∏—Å–∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("üîç –ê–ª–≥–æ—Ä–∏—Ç–º 4: –ü–æ–∏—Å–∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        
        try:
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            time.sleep(3)
            
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ª–µ–Ω–∏–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å lazy loading
            lazy_elements = self.driver.find_elements(By.CSS_SELECTOR, 
                '[data-src*=".pdf"], [data-lazy*=".pdf"], [loading="lazy"]')
            
            pdf_links = []
            for element in lazy_elements:
                # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –∫ —ç–ª–µ–º–µ–Ω—Ç—É
                self.driver.execute_script("arguments[0].scrollIntoView();", element)
                time.sleep(1)
                
                # –ü–æ–ª—É—á–∞–µ–º src –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
                src = element.get_attribute('src') or element.get_attribute('data-src')
                if src and '.pdf' in src.lower():
                    pdf_links.append(src)
            
            # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ iframe
            iframes = self.driver.find_elements(By.TAG_NAME, 'iframe')
            for iframe in iframes:
                try:
                    self.driver.switch_to.frame(iframe)
                    iframe_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*=".pdf"]')
                    for link in iframe_links:
                        href = link.get_attribute('href')
                        if href:
                            pdf_links.append(href)
                    self.driver.switch_to.default_content()
                except Exception as e:
                    self.driver.switch_to.default_content()
                    continue
            
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_links)} —Å—Å—ã–ª–æ–∫ –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ")
            return self._download_files(pdf_links, "algorithm_4")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ 4: {e}")
            return []
    
    def extract_pdfs_algorithm_5_form_submission(self, case_number):
        """–ê–ª–≥–æ—Ä–∏—Ç–º 5: –û—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ä–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è PDF"""
        logger.info("üîç –ê–ª–≥–æ—Ä–∏—Ç–º 5: –û—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ä–º")
        
        try:
            pdf_links = []
            
            # –ò—â–µ–º —Ñ–æ—Ä–º—ã —Å PDF
            forms = self.driver.find_elements(By.TAG_NAME, 'form')
            for form in forms:
                try:
                    # –ò—â–µ–º —Å–∫—Ä—ã—Ç—ã–µ –ø–æ–ª—è —Å PDF —Å—Å—ã–ª–∫–∞–º–∏
                    hidden_inputs = form.find_elements(By.CSS_SELECTOR, 'input[type="hidden"]')
                    for input_elem in hidden_inputs:
                        value = input_elem.get_attribute('value')
                        if value and '.pdf' in value.lower():
                            pdf_links.append(value)
                    
                    # –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–æ—Ä–º—É
                    submit_btn = form.find_element(By.CSS_SELECTOR, 'input[type="submit"], button[type="submit"]')
                    if submit_btn:
                        submit_btn.click()
                        time.sleep(3)
                        
                        # –ò—â–µ–º –Ω–æ–≤—ã–µ PDF —Å—Å—ã–ª–∫–∏ –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ñ–æ—Ä–º—ã
                        new_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*=".pdf"]')
                        for link in new_links:
                            href = link.get_attribute('href')
                            if href and href not in pdf_links:
                                pdf_links.append(href)
                                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–æ—Ä–º—ã: {e}")
                    continue
            
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_links)} —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ —Ñ–æ—Ä–º—ã")
            return self._download_files(pdf_links, "algorithm_5")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ 5: {e}")
            return []
    
    def extract_pdfs_algorithm_6_network_analysis(self, case_number):
        """–ê–ª–≥–æ—Ä–∏—Ç–º 6: –ê–Ω–∞–ª–∏–∑ —Å–µ—Ç–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞"""
        logger.info("üîç –ê–ª–≥–æ—Ä–∏—Ç–º 6: –ê–Ω–∞–ª–∏–∑ —Å–µ—Ç–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞")
        
        try:
            # –í–∫–ª—é—á–∞–µ–º –ø–µ—Ä–µ—Ö–≤–∞—Ç —Å–µ—Ç–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            self.driver.execute_cdp_cmd('Network.enable', {})
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–∑–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É PDF
            actions = [
                "window.scrollTo(0, document.body.scrollHeight);",
                "document.querySelectorAll('a').forEach(a => a.click());",
                "document.querySelectorAll('button').forEach(b => b.click());"
            ]
            
            pdf_links = []
            for action in actions:
                try:
                    self.driver.execute_script(action)
                    time.sleep(2)
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ç–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                    logs = self.driver.get_log('performance')
                    for log in logs:
                        try:
                            message = json.loads(log['message'])
                            if message['message']['method'] == 'Network.responseReceived':
                                url = message['message']['params']['response']['url']
                                if '.pdf' in url.lower():
                                    pdf_links.append(url)
                        except:
                            continue
                            
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–µ–π—Å—Ç–≤–∏—è: {e}")
                    continue
            
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_links)} —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å–µ—Ç–∏")
            return self._download_files(pdf_links, "algorithm_6")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ 6: {e}")
            return []
    
    def extract_pdfs_algorithm_7_content_parsing(self, case_number):
        """–ê–ª–≥–æ—Ä–∏—Ç–º 7: –ü–∞—Ä—Å–∏–Ω–≥ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        logger.info("üîç –ê–ª–≥–æ—Ä–∏—Ç–º 7: –ü–∞—Ä—Å–∏–Ω–≥ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç
            html_content = self.driver.page_source
            soup = BeautifulSoup(html_content, 'html.parser')
            
            pdf_links = []
            
            # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å PDF
            links = soup.find_all('a', href=re.compile(r'\.pdf', re.IGNORECASE))
            for link in links:
                href = link.get('href')
                if href:
                    pdf_links.append(href)
            
            # –ò—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
            elements_with_data = soup.find_all(attrs={'data-pdf': True})
            for element in elements_with_data:
                data_pdf = element.get('data-pdf')
                if data_pdf:
                    pdf_links.append(data_pdf)
            
            # –ò—â–µ–º –≤ JavaScript –∫–æ–¥–µ
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string:
                    js_matches = re.findall(r'["\']([^"\']*\.pdf[^"\']*)["\']', script.string)
                    pdf_links.extend(js_matches)
            
            # –ò—â–µ–º –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö HTML
            comments = soup.find_all(string=lambda text: isinstance(text, str) and '.pdf' in text)
            for comment in comments:
                matches = re.findall(r'https?://[^\s]+\.pdf', comment)
                pdf_links.extend(matches)
            
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_links)} —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥ HTML")
            return self._download_files(pdf_links, "algorithm_7")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ 7: {e}")
            return []
    
    def extract_pdfs_algorithm_8_advanced_selenium(self, case_number):
        """–ê–ª–≥–æ—Ä–∏—Ç–º 8: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã Selenium"""
        logger.info("üîç –ê–ª–≥–æ—Ä–∏—Ç–º 8: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã Selenium")
        
        try:
            pdf_links = []
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º ActionChains –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
            from selenium.webdriver.common.action_chains import ActionChains
            actions = ActionChains(self.driver)
            
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å hover —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏
            hover_elements = self.driver.find_elements(By.CSS_SELECTOR, 
                '[onmouseover], [onmouseenter], .hover, .tooltip')
            
            for element in hover_elements:
                try:
                    actions.move_to_element(element).perform()
                    time.sleep(1)
                    
                    # –ò—â–µ–º –ø–æ—è–≤–∏–≤—à–∏–µ—Å—è PDF —Å—Å—ã–ª–∫–∏
                    new_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*=".pdf"]')
                    for link in new_links:
                        href = link.get_attribute('href')
                        if href and href not in pdf_links:
                            pdf_links.append(href)
                            
                except Exception as e:
                    continue
            
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –¥–≤–æ–π–Ω—ã–º –∫–ª–∏–∫–æ–º
            double_click_elements = self.driver.find_elements(By.CSS_SELECTOR, 
                '[ondblclick], .double-click')
            
            for element in double_click_elements:
                try:
                    actions.double_click(element).perform()
                    time.sleep(2)
                    
                    # –ò—â–µ–º –Ω–æ–≤—ã–µ PDF —Å—Å—ã–ª–∫–∏
                    new_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*=".pdf"]')
                    for link in new_links:
                        href = link.get_attribute('href')
                        if href and href not in pdf_links:
                            pdf_links.append(href)
                            
                except Exception as e:
                    continue
            
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –ø—Ä–∞–≤—ã–º –∫–ª–∏–∫–æ–º
            right_click_elements = self.driver.find_elements(By.CSS_SELECTOR, 
                '[oncontextmenu], .context-menu')
            
            for element in right_click_elements:
                try:
                    actions.context_click(element).perform()
                    time.sleep(1)
                    
                    # –ò—â–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º –º–µ–Ω—é
                    context_links = self.driver.find_elements(By.CSS_SELECTOR, 
                        '.context-menu a[href*=".pdf"], .dropdown a[href*=".pdf"]')
                    for link in context_links:
                        href = link.get_attribute('href')
                        if href and href not in pdf_links:
                            pdf_links.append(href)
                            
                except Exception as e:
                    continue
            
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_links)} —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã")
            return self._download_files(pdf_links, "algorithm_8")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ 8: {e}")
            return []
    
    def _download_files(self, pdf_links, algorithm_name):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö PDF —Ñ–∞–π–ª–æ–≤"""
        downloaded_files = []
        
        for i, link in enumerate(pdf_links):
            try:
                # –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫—É
                if link.startswith('//'):
                    link = 'https:' + link
                elif link.startswith('/'):
                    link = self.driver.current_url.split('/')[0] + '//' + self.driver.current_url.split('/')[2] + link
                elif not link.startswith('http'):
                    link = urljoin(self.driver.current_url, link)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                filename = f"{algorithm_name}_file_{i+1}.pdf"
                filepath = os.path.join(self.files_dir, filename)
                
                # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
                response = requests.get(link, timeout=30)
                if response.status_code == 200 and response.content:
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ PDF
                    if filepath.endswith('.pdf') and len(response.content) > 1000:
                        downloaded_files.append(filepath)
                        logger.info(f"‚úÖ –°–∫–∞—á–∞–Ω: {filename} ({len(response.content)} –±–∞–π—Ç)")
                    else:
                        os.remove(filepath)
                        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {filename} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è PDF –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª")
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {link}: HTTP {response.status_code}")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {link}: {e}")
                continue
        
        return downloaded_files
    
    def run_all_algorithms(self, case_number):
        """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è PDF"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è PDF")
        
        all_downloaded_files = []
        algorithms = [
            self.extract_pdfs_algorithm_1_direct_links,
            self.extract_pdfs_algorithm_2_javascript_links,
            self.extract_pdfs_algorithm_3_api_calls,
            self.extract_pdfs_algorithm_4_dynamic_content,
            self.extract_pdfs_algorithm_5_form_submission,
            self.extract_pdfs_algorithm_6_network_analysis,
            self.extract_pdfs_algorithm_7_content_parsing,
            self.extract_pdfs_algorithm_8_advanced_selenium
        ]
        
        for i, algorithm in enumerate(algorithms, 1):
            try:
                logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ {i}/8")
                files = algorithm(case_number)
                all_downloaded_files.extend(files)
                logger.info(f"‚úÖ –ê–ª–≥–æ—Ä–∏—Ç–º {i} –∑–∞–≤–µ—Ä—à–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ {i}: {e}")
                continue
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_files = list(set(all_downloaded_files))
        logger.info(f"üéâ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(unique_files)}")
        
        return unique_files
